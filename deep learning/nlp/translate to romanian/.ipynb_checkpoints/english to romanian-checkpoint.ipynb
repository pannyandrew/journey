{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c2c347",
   "metadata": {},
   "source": [
    "# Translating English to Romanian with a RNN\n",
    "I'm trying to get a better understanding of RNN's before I move to transformers so I will be implementing a RNN that translates english to romanian!  \n",
    "I will be following this [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) but will train it to translate it to romanian. Afterwards, I want to ask my model questions in English and have it respond in Japanese."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880e3ee",
   "metadata": {},
   "source": [
    "## Table of Contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "209c22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be86a543",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Our data is from https://www.manythings.org/anki/ and is a text file.  The file is a tab separated list of translation pairs: `Hi.\tもしもし`.\n",
    "\n",
    "We will represent every word in our language as a one-hot vector. We'll need a unique index per word to use as the input and targets of our network.  \n",
    "Our Lang class will keep track of word to index as well as index to word, and we'll keep track of the number of words and use the final index as the index of rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ccf19c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c03b5",
   "metadata": {},
   "source": [
    "The files are in unicode. To simplify the files, we will convert them to ASCII, make everything lowercase, and trim most of the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3938420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0f7d7",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "There are a lot of example sentences so we'll only take the smaller sentences.\n",
    "\n",
    "We're filtering so that the length of the of the sentences is less than 10 and they only start with certain prefixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "54004647",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 10\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14b4f5",
   "metadata": {},
   "source": [
    "## Reading the Data\n",
    "To read the file, we'll split the file into lines, then split the lines into pairs; we'll also add a reverse function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "018f963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s-%s/ron.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    pairs = [p[:2] for p in pairs]\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3b91f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs('ron', 'eng', reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8e171188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 14237 sentence pairs\n",
      "Trimmed to 11339 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 6602\n",
      "ron 4717\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('ron', 'eng', reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b64985",
   "metadata": {},
   "source": [
    "# Seq2Seq Model\n",
    "Seq2Seq models are models consisting of two RNN's: an encoder and a decoder. The encoder reads a sequence and outputs a single vector, the decoder reads that vector to produce an output sequence.\n",
    "\n",
    "When you translate words directly from one language to another, the meaning is sometimes lost because the words are in different orders. This means it's difficult to produce a correct translation from just a sequence of words.  \n",
    "We feed the sequence into an encoder, which ideally encodes the *meaning* of the input sentence into a single vector.\n",
    "\n",
    "## The Encoder\n",
    "The encoder outputs some value for every word in the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state as input for the next input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "a5257db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1eecac",
   "metadata": {},
   "source": [
    "## The Decoder\n",
    "The decoder takes the encoder output vectors and outputs a sequence of words to create the translation\n",
    "\n",
    "### Simple Decoder\n",
    "In the simplest Seq2Seq decoder, we only use the last output of the encoder, sometimes referred to as the *context vector* as it encodes context from the entire sequence. This context vector is used as the initial input for the hidden state of the decoder.  \n",
    "\n",
    "At every step of decoding, the decoder is given an input token and a hidden state. The initial input token is the *SOS* token and the initial hidden state is the *context vector*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "4f976f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(self, DecoderRNN).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, 1, -1)\n",
    "        output = F.ReLU(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        output = self.softmax(self.out(output[0]))\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd849388",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disease_class",
   "language": "python",
   "name": "disease_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
