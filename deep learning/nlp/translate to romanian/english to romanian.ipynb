{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51c2c347",
   "metadata": {},
   "source": [
    "# Translating English to Romanian with a RNN\n",
    "I'm trying to get a better understanding of RNN's before I move to transformers so I will be implementing a RNN that translates english to romanian!  \n",
    "I will be following this [tutorial](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) but will train it to translate it to romanian. Afterwards, I want to ask my model questions in English and have it respond in Japanese."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880e3ee",
   "metadata": {},
   "source": [
    "## Table of Contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209c22db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be86a543",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "Our data is from https://www.manythings.org/anki/ and is a text file.  The file is a tab separated list of translation pairs: `Hi.\tもしもし`.\n",
    "\n",
    "We will represent every word in our language as a one-hot vector. We'll need a unique index per word to use as the input and targets of our network.  \n",
    "Our Lang class will keep track of word to index as well as index to word, and we'll keep track of the number of words and use the final index as the index of rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccf19c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  \n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00c03b5",
   "metadata": {},
   "source": [
    "The files are in unicode. To simplify the files, we will convert them to ASCII, make everything lowercase, and trim most of the punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3938420a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf0f7d7",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "There are a lot of example sentences so we'll only take the smaller sentences.\n",
    "\n",
    "We're filtering so that the length of the of the sentences is less than 10 and they only start with certain prefixes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54004647",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 7\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14b4f5",
   "metadata": {},
   "source": [
    "## Reading the Data\n",
    "To read the file, we'll split the file into lines, then split the lines into pairs; we'll also add a reverse function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "018f963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    lines = open('%s-%s/ron.txt' % (lang1, lang2), encoding='utf-8').\\\n",
    "        read().strip().split('\\n')\n",
    "\n",
    "    # Split every line into pairs and normalize\n",
    "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
    "    pairs = [p[:2] for p in pairs]\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b91f958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs('ron', 'eng', reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "    return input_lang, output_lang, pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e171188",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "Read 14237 sentence pairs\n",
      "Trimmed to 5385 sentence pairs\n",
      "Counting words...\n",
      "Counted words:\n",
      "eng 3778\n",
      "ron 2931\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = prepare_data('ron', 'eng', reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fe5ab0",
   "metadata": {},
   "source": [
    "# Seq2Seq Model\n",
    "Seq2Seq models are models consisting of two RNN's: an encoder and a decoder. The encoder reads a sequence and outputs a single vector, the decoder reads that vector to produce an output sequence.\n",
    "\n",
    "When you translate words directly from one language to another, the meaning is sometimes lost because the words are in different orders. This means it's difficult to produce a correct translation from just a sequence of words.  \n",
    "We feed the sequence into an encoder, which ideally encodes the *meaning* of the input sentence into a single vector.\n",
    "\n",
    "## The Encoder\n",
    "The encoder outputs some value for every word in the input sentence. For every input word the encoder outputs a vector and a hidden state, and uses the hidden state as input for the next input word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dc4d580",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        output = embedded\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a129dd",
   "metadata": {},
   "source": [
    "## The Decoder\n",
    "The decoder takes the encoder output vectors and outputs a sequence of words to create the translation\n",
    "\n",
    "### Attention Decoder\n",
    "When only the context vector is passed between the encoder and decoder, that single vector carries the burden of encoding the entire sentence.  \n",
    "Attention allows the decoder network to *focus* on a different part of the encoders output for every step of the decoders own outputs. \n",
    "First, we calculate a set of **attention weights**. These will be multiplied by the encoders output to create a weighted combination, the result: `attn_applied` should contain information about that specific part of the input sequence and help the decoder choose the right output words. \n",
    "\n",
    "Calculating the attention weights is done with another feed-forward network: `attn`, using the decoder's input and hidden state as input. There are sentences of all sizes in the training data so we have to choose a max length (input length, for encoder outputs) it can apply to. Sentences of max length will use all the attention weights, while shorter sentences will only use the first few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54444301",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1cbb6d",
   "metadata": {},
   "source": [
    "# Training\n",
    "## Preparing Training Data\n",
    "For each pair, we need an input tensor (indexes of words from the input) and a target tensor (indexes of words from the target). While creating these tensors, we append the *EOS* token to the end of each tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "790c0843",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idxFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = idxFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long).view(-1,1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    output_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return (input_tensor, output_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7759ab5",
   "metadata": {},
   "source": [
    "## Training \n",
    "To train the model, we run the input through the encoder and keep track of every output and the latest hidden state. Then the decoder is given the *SOS* token as its first input and the last hidden state of the encoder as the first hidden state.\n",
    "\n",
    "The concept of **Teacher Forcing** uses the real target outputs as each next input, instead of using the decoder's guess as the next input. This causes the network to converge faster, [but can cause some instability when the trained network is exploited.](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf)  \n",
    "\n",
    "This means you can observe outputs with correct grammar, but the wrong translation. Intuitively, the model has learned to represent the output grammar and can \"pick up\" the meaning after the teacher tells it the first few words, but it hasn't learned how to properly create the sentence from the translation in the first place. \n",
    "\n",
    "We will be turning on/off our teacher randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46d736b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
    "    encoder_hidden = encoder.initHidden()\n",
    "\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    input_length = input_tensor.size(0)\n",
    "    target_length = target_tensor.size(0)\n",
    "\n",
    "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "    loss = 0\n",
    "\n",
    "    for ei in range(input_length):\n",
    "        encoder_output, encoder_hidden = encoder(\n",
    "            input_tensor[ei], encoder_hidden)\n",
    "        encoder_outputs[ei] = encoder_output[0, 0]\n",
    "\n",
    "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
    "\n",
    "    decoder_hidden = encoder_hidden\n",
    "\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    if use_teacher_forcing:\n",
    "        # Teacher forcing: Feed the target as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            decoder_input = target_tensor[di]  # Teacher forcing\n",
    "\n",
    "    else:\n",
    "        # Without teacher forcing: use its own predictions as the next input\n",
    "        for di in range(target_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "            loss += criterion(decoder_output, target_tensor[di])\n",
    "            if decoder_input.item() == EOS_token:\n",
    "                break\n",
    "\n",
    "    loss.backward()\n",
    "\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f8eb95",
   "metadata": {},
   "source": [
    "Helper function to print time elapsed and estimated time remaining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "994c6625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734a1be4",
   "metadata": {},
   "source": [
    "## Training Process\n",
    "The training process looks like this: \n",
    "* start a timer\n",
    "* initialize parameters and criterion\n",
    "* create a set of training pairs\n",
    "* start empty losses array for plotting\n",
    "* call train many times and print the progress occasionally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "870d15b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0 # reset every print_every\n",
    "    plot_loss_total = 0 # reset every plot_every\n",
    "    \n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "    training_pairs = [tensorsFromPair(random.choice(pairs)) for i in range(n_iters)]\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    for iter in range(1, n_iters + 1):\n",
    "        training_pair = training_pairs[iter - 1]\n",
    "        input_tensor = training_pair[0]\n",
    "        target_tensor = training_pair[1]\n",
    "        \n",
    "        loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "        \n",
    "        if iter % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
    "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
    "        \n",
    "        if iter % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    \n",
    "    showPlot(plot_losses)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5edfb24",
   "metadata": {},
   "source": [
    "## Plotting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "599e856f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('TkAgg')\n",
    "from tkinter import *\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392339e8",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "Mostly the same as training, but there are no targets so we feed the decoder predictions back to itself for each step. Every time it predicts a word we add it to the output string and when it predicts the *EOS* token we stop there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7ec071d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "        input_length = input_tensor.size()[0]\n",
    "        encoder_hidden = encoder.initHidden()\n",
    "\n",
    "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
    "\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
    "                                                     encoder_hidden)\n",
    "            encoder_outputs[ei] += encoder_output[0, 0]\n",
    "\n",
    "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
    "\n",
    "        decoder_hidden = encoder_hidden\n",
    "\n",
    "        decoded_words = []\n",
    "        decoder_attentions = torch.zeros(max_length, max_length)\n",
    "\n",
    "        for di in range(max_length):\n",
    "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs)\n",
    "            decoder_attentions[di] = decoder_attention.data\n",
    "            topv, topi = decoder_output.data.topk(1)\n",
    "            if topi.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            else:\n",
    "                decoded_words.append(output_lang.index2word[topi.item()])\n",
    "\n",
    "            decoder_input = topi.squeeze().detach()\n",
    "\n",
    "        return decoded_words, decoder_attentions[:di + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9ab71b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51bf9f0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1m 29s (- 20m 55s) (5000 6%) 3.9767\n",
      "2m 57s (- 19m 14s) (10000 13%) 3.1773\n",
      "4m 26s (- 17m 46s) (15000 20%) 2.5984\n",
      "5m 51s (- 16m 6s) (20000 26%) 2.1739\n",
      "7m 16s (- 14m 32s) (25000 33%) 1.7677\n",
      "8m 40s (- 13m 1s) (30000 40%) 1.4406\n",
      "10m 5s (- 11m 31s) (35000 46%) 1.1941\n",
      "11m 29s (- 10m 3s) (40000 53%) 0.9360\n",
      "12m 53s (- 8m 35s) (45000 60%) 0.7426\n",
      "14m 18s (- 7m 9s) (50000 66%) 0.5910\n",
      "15m 42s (- 5m 42s) (55000 73%) 0.4765\n",
      "17m 8s (- 4m 17s) (60000 80%) 0.4060\n",
      "18m 34s (- 2m 51s) (65000 86%) 0.3078\n",
      "20m 1s (- 1m 25s) (70000 93%) 0.2599\n",
      "21m 29s (- 0m 0s) (75000 100%) 0.2127\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 256\n",
    "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
    "\n",
    "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f289c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> nu ti asuma riscuri .\n",
      "= don t take any chances .\n",
      "< don t take any chances . <EOS>\n",
      "\n",
      "> tom a curatat pestele .\n",
      "= tom cleaned the fish .\n",
      "< tom cleaned the fish . <EOS>\n",
      "\n",
      "> am platit in avans .\n",
      "= i paid in advance .\n",
      "< i paid in advance . <EOS>\n",
      "\n",
      "> noi plecam .\n",
      "= we re going .\n",
      "< we re going . <EOS>\n",
      "\n",
      "> sunteti cruda .\n",
      "= you re cruel .\n",
      "< you re cruel . <EOS>\n",
      "\n",
      "> el stia sa vorbeasca franceza .\n",
      "= he could speak french .\n",
      "< he could speak french . <EOS>\n",
      "\n",
      "> a venit vara .\n",
      "= summer has come .\n",
      "< summer has come . <EOS>\n",
      "\n",
      "> deja am ales .\n",
      "= i ve already chosen .\n",
      "< i ve already chosen . <EOS>\n",
      "\n",
      "> tu ai un zambet frumos .\n",
      "= you have a beautiful smile .\n",
      "< you have a beautiful smile . <EOS>\n",
      "\n",
      "> tom o va gasi .\n",
      "= tom will find it .\n",
      "< tom will find it . <EOS>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluateRandomly(encoder1, attn_decoder1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6721ac9",
   "metadata": {},
   "source": [
    "## Visualizing Attention\n",
    "A useful property of attention is its highly interpretable outputs. Because it is used to weight specific encoder outputs of the input sequence, we can imagine where the network is focused most at each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4ff6bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    # Set up figure with colorbar\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(\n",
    "        encoder1, attn_decoder1, input_sentence)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "    showAttention(input_sentence, output_words, attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd653d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input = eu fac naveta cu trenul .\n",
      "output = i commute by train . <EOS>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_7/749pk1cs14scrc516s0bx0cc0000gn/T/ipykernel_50791/3159908345.py:9: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
      "/var/folders/_7/749pk1cs14scrc516s0bx0cc0000gn/T/ipykernel_50791/3159908345.py:11: UserWarning: FixedFormatter should only be used together with FixedLocator\n",
      "  ax.set_yticklabels([''] + output_words)\n"
     ]
    }
   ],
   "source": [
    "evaluateAndShowAttention(\"eu fac naveta cu trenul .\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c05a53d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "disease_class",
   "language": "python",
   "name": "disease_class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
